<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HTML解析大法</title>
    <url>/2022/01/17/HTML%E8%A7%A3%E6%9E%90%E5%A4%A7%E6%B3%95/</url>
    <content><![CDATA[<h1 id="HTML解析大法："><a href="#HTML解析大法：" class="headerlink" title="HTML解析大法："></a>HTML解析大法：</h1><h2 id="一-正则表达式"><a href="#一-正则表达式" class="headerlink" title="一.正则表达式"></a>一.正则表达式</h2><h3 id="1-模块主要函数"><a href="#1-模块主要函数" class="headerlink" title="1.模块主要函数"></a>1.模块主要函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-comment">#创建正则表达式对象</span><br>re.<span class="hljs-built_in">compile</span>(pattern)<br><span class="hljs-comment">#在字符串中寻找模式，返回match对象或None</span><br>re.search(pattern,string)<br><span class="hljs-comment">#从字符串开头匹配，返回match或None</span><br>re.match(pattern,string)<br><span class="hljs-comment">#返回列表</span><br>re.findall(pattern,string)<br></code></pre></td></tr></table></figure>

<h3 id="2-使用正则表达式对象"><a href="#2-使用正则表达式对象" class="headerlink" title="2.使用正则表达式对象"></a>2.使用正则表达式对象</h3><p>使用<code>re.compile</code>将正则表达式编译成正则表达式对象，即可使用对象的方法（与上文search、match、findall用法大同小异）</p>
<h3 id="3-子模式与Match对象"><a href="#3-子模式与Match对象" class="headerlink" title="3.子模式与Match对象"></a>3.子模式与Match对象</h3><p>Match对象主要方法有**group()<strong>（返回模式子内容），</strong>groups()<strong>（返回模式字内容元组），</strong>groupdict()**（返回字典）</p>
<p><strong>特别地，使用(?P&lt;name&gt;)可以为子模式命名</strong></p>
<h2 id="二-BeautifulSoup"><a href="#二-BeautifulSoup" class="headerlink" title="二.BeautifulSoup"></a>二.BeautifulSoup</h2><h3 id="1-对象种类"><a href="#1-对象种类" class="headerlink" title="1.对象种类"></a>1.对象种类</h3><h4 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h4><p>即HTML中尖括号扩起来的部分。其有两个重要的属性：<strong>name</strong>和<strong>attributes</strong><br><strong>name</strong>属性：<br>每个Tag的名字，包括<strong>title、a、h1</strong>等等。<strong>name</strong>属性可以被修改。<br> <strong>attributes</strong>属性：<br>每个Tag中包含等号，等号两边的内容（像字典一样）就是是attribute，可以使用类似python操作字典的方法操作属性，建议使用get。</p>
<h4 id="②NavigableString"><a href="#②NavigableString" class="headerlink" title="②NavigableString"></a>②NavigableString</h4><p>对Tag对象使用.<strong>string</strong>获得其中文字<br>####③BeautifulSoup<br>与Tag对象类似，只不过可以把<strong>BeautifulSoup</strong>对象看成整体的Tag</p>
<h4 id="④comment"><a href="#④comment" class="headerlink" title="④comment"></a>④comment</h4><p>即HTML中的注释部分。在使用.<strong>string</strong>时会同时提取注释，所以使用时可以先判断类型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(soup.a.string) == bs4.element.comment:<br>    <span class="hljs-built_in">print</span>(soupa.a.string)<br></code></pre></td></tr></table></figure>

<h3 id="2-BeautifulSoup的树结构"><a href="#2-BeautifulSoup的树结构" class="headerlink" title="2.BeautifulSoup的树结构"></a>2.BeautifulSoup的树结构</h3><h4 id="子节点"><a href="#子节点" class="headerlink" title="子节点"></a>子节点</h4><p>.<strong>content</strong>以及.<strong>children</strong>都可返回直接子节点，不同的是前者返回的是列表。<br>.<strong>descendants</strong>可以返回所有的子孙节点（返回的是可迭代对象）。</p>
<h4 id="父节点"><a href="#父节点" class="headerlink" title="父节点"></a>父节点</h4><p>.<strong>parents</strong>返回父节点的可迭代对象。</p>
<h4 id="兄弟节点"><a href="#兄弟节点" class="headerlink" title="兄弟节点"></a>兄弟节点</h4><p>.<strong>next_sibling</strong>和.<strong>previous_sibing</strong>返回前一个和后一个兄弟节点。</p>
<h3 id="find-all方法"><a href="#find-all方法" class="headerlink" title="find_all方法"></a>find_all方法</h3><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">find_all(name, attrs, recursive, text, **kwarg)<br></code></pre></td></tr></table></figure>
<h4 id="name参数："><a href="#name参数：" class="headerlink" title="name参数："></a>name参数：</h4><p>寻找名字为name的标记</p>
<h4 id="kwargs"><a href="#kwargs" class="headerlink" title="kwargs"></a>kwargs</h4><p>查找参数符合条件的标记<br>例如：<code>find_all(&#39;a&#39;,class_=&#39;post-title&#39;)</code>寻找参数class为post-title的标记</p>
<h4 id="text"><a href="#text" class="headerlink" title="text"></a>text</h4><p>搜索文档中的字符串内容。</p>
<h4 id="limit"><a href="#limit" class="headerlink" title="limit"></a>limit</h4><p>限制搜索结果的个数。</p>
<h4 id="recursive"><a href="#recursive" class="headerlink" title="recursive"></a>recursive</h4><p>默认为true，若为false，则只寻找当前节点的直接子节点。</p>
<h2 id="三-lxml的xpath解析"><a href="#三-lxml的xpath解析" class="headerlink" title="三.lxml的xpath解析"></a>三.lxml的xpath解析</h2><p>lxml库的使用方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">rest = etree.HTML(html)<br></code></pre></td></tr></table></figure>
<h3 id="XPath节点"><a href="#XPath节点" class="headerlink" title="XPath节点"></a>XPath节点</h3><p>XPath的节点关系类似BeautifulSoup，都是树的结构，此处不再赘述。</p>
<h3 id="XPath语法"><a href="#XPath语法" class="headerlink" title="XPath语法"></a>XPath语法</h3><table>
<thead>
<tr>
<th>表达式</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td>nodename</td>
<td align="center">选取此节点的所有子节点</td>
</tr>
<tr>
<td>/</td>
<td align="center">从根节点选取</td>
</tr>
<tr>
<td>//</td>
<td align="center">选择任意位置的某个节点</td>
</tr>
<tr>
<td>.</td>
<td align="center">选取当前节点</td>
</tr>
<tr>
<td>..</td>
<td align="center">选取当前节点的父节点</td>
</tr>
<tr>
<td>@</td>
<td align="center">选取</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>谓语</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td>/classroom/student[1]</td>
<td align="center">选取classroom子元素的第一个student元素</td>
</tr>
<tr>
<td>/classroom/student[last()]</td>
<td align="center">选取classroom子元素中的最后一个student元素</td>
</tr>
<tr>
<td>/classroom/student[position()&lt;6]</td>
<td align="center">选取classroom子元素的前五个元素</td>
</tr>
<tr>
<td>//name[@lang]</td>
<td align="center">选取name元素，其必须有lang属性</td>
</tr>
<tr>
<td>//name[@lang=’en’]</td>
<td align="center">选取name元素，其lang属性的值必须为en</td>
</tr>
<tr>
<td>//student[age&gt;20]</td>
<td align="center">选取所有student，其age属性必须大于20</td>
</tr>
</tbody></table>
<p>除此之外Xpath还可以在表达式中使用/text()来提取其中的字符（类似bs的.string）<br>Selector对象还有extract()方法，获取其中的文字。</p>
<p>###偷懒小技巧<br>在浏览器的审查元素中可以右键对元素的XPath进行复制（仅适用于提取单个元素的情况）</p>
]]></content>
      <tags>
        <tag>前端</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>scrapy爬虫框架学习笔记</title>
    <url>/2022/01/23/scrapy%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Scrapy入门"><a href="#Scrapy入门" class="headerlink" title="Scrapy入门"></a>Scrapy入门</h1><h2 id="一-基础指令"><a href="#一-基础指令" class="headerlink" title="一.基础指令"></a>一.基础指令</h2><p><strong>使用命令提示符，cd到想要操作的目录</strong></p>
<h3 id="scrapy-startproject-name"><a href="#scrapy-startproject-name" class="headerlink" title="scrapy startproject name"></a>scrapy startproject name</h3><p>在该目录下创建项目</p>
<h3 id="scrapy-genspider-name-domain"><a href="#scrapy-genspider-name-domain" class="headerlink" title="scrapy genspider name domain"></a>scrapy genspider name domain</h3><p><strong>使用前需要先cd到下一级目录</strong></p>
<blockquote>
<p>domain参数不需要带http或者https的头，也不需要引号</p>
</blockquote>
<h3 id="scrapy-crawl-name"><a href="#scrapy-crawl-name" class="headerlink" title="scrapy crawl name"></a>scrapy crawl name</h3><p>启动名字为name的爬虫</p>
<hr>
<h2 id="二-开始"><a href="#二-开始" class="headerlink" title="二.开始"></a>二.开始</h2><h3 id="在name-py中进行编辑"><a href="#在name-py中进行编辑" class="headerlink" title="在name.py中进行编辑"></a>在name.py中进行编辑</h3><p>爬虫开启后会把html传到name.py中的parse方法，可以在parse中进行网页解析。<br>解析可以使用scrapy自带的方法，也可以脱离框架使用BeautifulSoup、XPath、CSS等。</p>
<h3 id="日志信息的屏蔽"><a href="#日志信息的屏蔽" class="headerlink" title="日志信息的屏蔽"></a>日志信息的屏蔽</h3><p>如果没有对日志信息屏蔽，会产生大量信息，影响调试。可以在settings.py中加入以下代码，将等级warning以下的日志信息全部屏蔽</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">LOG_LEVEL = <span class="hljs-string">&quot;WARNING&quot;</span><br></code></pre></td></tr></table></figure>
<hr>
<h2 id="三-将数据传输到pipe中"><a href="#三-将数据传输到pipe中" class="headerlink" title="三.将数据传输到pipe中"></a>三.将数据传输到pipe中</h2><p><strong>使用yield关键字</strong><br>yield可以将数据逐个转入，可以使用的数据类型是Request,BaseItem,dict或者None。（不可以直接返回列表！！）</p>
<h3 id="在此之前"><a href="#在此之前" class="headerlink" title="在此之前"></a>在此之前</h3><p>使用pipe需要先在设置中开启pipeline，在setting.py中取消列代码的注释：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">ITEM_PIPELINES = &#123;<br>   <span class="hljs-string">&#x27;firstscrapy.pipelines.FirstscrapyPipeline&#x27;</span>: <span class="hljs-number">300</span>,<br>&#125;<br></code></pre></td></tr></table></figure>
<p>字典中的键表示pipeline的名字，值表示这个pipeline的权重，数据会先结果权重小的pipeline再经过权重大的。</p>
<h3 id="为什么要有多个pipeline"><a href="#为什么要有多个pipeline" class="headerlink" title="为什么要有多个pipeline"></a>为什么要有多个pipeline</h3><ul>
<li>一个项目可能有多个爬虫，爬取不同的网站产生不同的数据，所以需要多个pipeline对数据进行处理。</li>
<li>当然也可以使用一个pipeline，通过对item加一个<strong>come_from</strong>的键来判断该是数据的来源</li>
</ul>
<h3 id="pipeline-process-item方法的第三个参数spider"><a href="#pipeline-process-item方法的第三个参数spider" class="headerlink" title="pipeline.process_item方法的第三个参数spider"></a>pipeline.process_item方法的第三个参数spider</h3><p>spider参数即为创建的爬虫中的类，其有name属性（详见创建的myspider.py），可以通过name来判断数据的来源，从而对不同数据进行处理。</p>
<hr>
<h2 id="四-logging的使用"><a href="#四-logging的使用" class="headerlink" title="四.logging的使用"></a>四.logging的使用</h2><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br>logging.warning(item)<br></code></pre></td></tr></table></figure>
<p>  将item以warning的方式打印出来。与print不同的地方是，logging可以以更高的等级打印，而且可以将打印的内容保存到本地，方便后续的调试，为开发提供了方便。<br>这里使用warning是因为之前在settings.py中设置了打印的等级为warning，只有warning级别的日志才能被打印在命令提示符里。</p>
<h3 id="指明输出日志的来源"><a href="#指明输出日志的来源" class="headerlink" title="指明输出日志的来源"></a>指明输出日志的来源</h3><hr>
<hr>
<h1 id="组装Request对象"><a href="#组装Request对象" class="headerlink" title="组装Request对象"></a>组装Request对象</h1><p><strong>语法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">yield</span> scrapy.Rquest(next_page_url, call_back=self.parse)<br></code></pre></td></tr></table></figure>
<p><strong>在settings中设置User-Agent：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">USER-AGENT = <span class="hljs-string">&#x27;Mozilla/5.0&#x27;</span><br></code></pre></td></tr></table></figure>

<h2 id="完整的scrapy-Request-对象"><a href="#完整的scrapy-Request-对象" class="headerlink" title="完整的scrapy.Request()对象"></a>完整的scrapy.Request()对象</h2><p>scrapy.Request(url[,callback,method=’GET’,headers,body,cookies,meta,dont_filter=False])</p>
<ul>
<li>注意事项<ul>
<li>callback：制定传入的url参数交给哪个解析函数去处理</li>
<li>meta：实现在不同解析函数中传递数据，meta默认会携带部分信息，如下载延迟，请求深度等等。<strong>meta以字典形式定义</strong>，存取数据均与字典操作相同。</li>
<li>dont_filter：让scrapy不再对url去重，即一个url地址可能访问多次。</li>
<li>cookies只能储存在函数的参数中，将cookies用字符串形式放在headers中传入将不起作用。</li>
</ul>
</li>
</ul>
<hr>
<hr>
<p>直接使用上面的语句，输出的日志前面总是带有一个[root]标识，这表示了日志的来源，实际上，我们希望每一句日志都能体现出其来源，所以要使用下面的语句。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">logger = logging.getlogger(__name__)<br></code></pre></td></tr></table></figure>
<p>该函数实例化一个logging，就可以使用下面的语句输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">logger.warning(item)<br></code></pre></td></tr></table></figure>
<p>就可以在输出的日志前面加上[项目.spiders.爬虫名]的提示信息</p>
<h3 id="日志的保存"><a href="#日志的保存" class="headerlink" title="日志的保存"></a>日志的保存</h3><p>在settings.py中设置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">LOG_FILE = <span class="hljs-string">&quot;./log.log&quot;</span><br></code></pre></td></tr></table></figure>
<p>第一个.号表示当前目录，储存后的文件为log.log。<br>修改设置后日志不会在命令提示符中输出。</p>
<hr>
<hr>
<h1 id="选择器（html解析器）"><a href="#选择器（html解析器）" class="headerlink" title="选择器（html解析器）"></a>选择器（html解析器）</h1><blockquote>
<p>scrapy拥有自己的一套数据提取机制，成为选择器（selector），因为它们通过特定的xpath或者css表达式来选择HTML文件中的某个部分。scrapy选择器构建于lxml库之上，这意味着它们在速度和解析准确性上非常相似，用法也和之前讲的lxml解析基本相似。当然也可以脱离这些体系，使用bs4库进行解析。</p>
</blockquote>
<hr>
<h2 id="selector的用法"><a href="#selector的用法" class="headerlink" title="selector的用法"></a>selector的用法</h2><h3 id="1-xpath-："><a href="#1-xpath-：" class="headerlink" title="1.xpath()："></a>1.xpath()：</h3><p>传入Xpath表达式，返回该表达式所对应的所有节点的selector list<strong>列表</strong></p>
<h3 id="2-css"><a href="#2-css" class="headerlink" title="2.css()"></a>2.css()</h3><p>传入css表达式，返回该表达式所对应的所有节点的selector list<strong>列表</strong></p>
<h3 id="3-extract"><a href="#3-extract" class="headerlink" title="3.extract()"></a>3.extract()</h3><p>序列化该节点为Unicode字符串并返回list<strong>列表</strong></p>
<h3 id="re"><a href="#re" class="headerlink" title="re()"></a>re()</h3><p>根据传入的正则表达式对数据进行提取，返回Unicode字符串列表，regex可以是一个已经编译的正则表达式，也可以是一个即将被编译的正则表达式字符串</p>
<hr>
<h2 id="更好地配对"><a href="#更好地配对" class="headerlink" title="更好地配对"></a>更好地配对</h2><p>方法的调用很简单，更多的时间是花在Xpath和css表达式的构造。scrapy提供了一种简便的方法来查看表达式是否正确、是否真的起作用。<br>另起一个命令行窗口，在其中输入</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">scrapy </span><span class="hljs-keyword">shell </span><span class="hljs-string">&quot;/爬取网站的网址&quot;</span><br></code></pre></td></tr></table></figure>
<p><strong>记得要加双引号</strong><br>然后输入response.xpath(“…”)或者response.css(“…”)就可以抽取出当前网站的标签（返回的是Unicode格式）。</p>
<hr>
<hr>
<h1 id="通过POST请求登录"><a href="#通过POST请求登录" class="headerlink" title="通过POST请求登录"></a>通过POST请求登录</h1><h2 id="一、通过FormRequest类来发送请求（类似Request）"><a href="#一、通过FormRequest类来发送请求（类似Request）" class="headerlink" title="一、通过FormRequest类来发送请求（类似Request）"></a>一、通过FormRequest类来发送请求（类似Request）</h2><p>首先查看network中发送请求的请求头，获取请求头中的类似用户名、密码的键值对<br>特别要注意cookie、token等值，要在源代码中找到对应的值，在python的字典中填入。</p>
<h2 id="二、更方便的方法"><a href="#二、更方便的方法" class="headerlink" title="二、更方便的方法"></a>二、更方便的方法</h2><p>FormRequest类有一个from_request的方法，可以自动寻找网页中的表单。</p>
<h3 id="参数："><a href="#参数：" class="headerlink" title="参数："></a>参数：</h3><ul>
<li>response</li>
<li>formdata 字典</li>
<li>callback <h2 id="如果response中有两个表单怎么办？"><a href="#如果response中有两个表单怎么办？" class="headerlink" title="#如果response中有两个表单怎么办？"></a>#如果response中有两个表单怎么办？</h2>可以查看该方法的源代码，通过给其传入xpath、css等参数来定位表单。</li>
</ul>
<h2 id="三、直接通过Request类来发送请求，不过要将其中的method参数改为POST"><a href="#三、直接通过Request类来发送请求，不过要将其中的method参数改为POST" class="headerlink" title="三、直接通过Request类来发送请求，不过要将其中的method参数改为POST"></a>三、直接通过Request类来发送请求，不过要将其中的method参数改为POST</h2><hr>
<hr>
<h1 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h1><h2 id="下载中间件"><a href="#下载中间件" class="headerlink" title="下载中间件"></a>下载中间件</h2><h3 id="使用方法和pipeline类似，在文件中定义一个类，然后在setting中开启"><a href="#使用方法和pipeline类似，在文件中定义一个类，然后在setting中开启" class="headerlink" title="使用方法和pipeline类似，在文件中定义一个类，然后在setting中开启"></a>使用方法和pipeline类似，在文件中定义一个类，然后在setting中开启</h3><h3 id="默认方法"><a href="#默认方法" class="headerlink" title="默认方法"></a>默认方法</h3><p><strong>process_request(self,request,spider)</strong><br>当每个request通过下载中间件时，该方法被调用，可以用来修改请求头等信息（请求时调用）。该方法只能返回None。<br>返回值：</p>
<ul>
<li>若返回None，Scrapy将继续处理该Request，执行其他的中间件的相应方法，直到合适的下载器处理函数被调用，该Request被执行</li>
<li>若返回Response对象，Scrapy不会调用其他的方法，而直接返回该response。</li>
<li>若返回Request对象，Scrapy将停止调用其他中间件的process_request，并重新调度返回的Request。</li>
</ul>
<p><strong>process_response(self,request,response,spider)</strong><br>当下载器完成http请求，传递响应给引擎时调用（获得请求时调用）。该方法必须return request或者response。<br>返回值：</p>
<ul>
<li>当返回Response对象时，会将该response交给其他中间件的proce_response处理</li>
<li>当返回Request对象是，则中间件链停止，返回的Request会被重新调度并下载。</li>
</ul>
<h3 id="process-request还可以用于添加代理"><a href="#process-request还可以用于添加代理" class="headerlink" title="process_request还可以用于添加代理"></a>process_request还可以用于添加代理</h3><p>用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">request.meta[<span class="hljs-string">&quot;proxy&quot;</span>] = <span class="hljs-string">&quot;http://124.115.126.76:808&quot;</span><br><span class="hljs-comment">#代理的形式为：协议+ip地址+端口</span><br></code></pre></td></tr></table></figure>
<hr>
<hr>
<h1 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h1><h2 id="在文件items-py中编辑"><a href="#在文件items-py中编辑" class="headerlink" title="在文件items.py中编辑"></a>在文件items.py中编辑</h2><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FirstscrapyItem</span>(<span class="hljs-params">scrapy.Item</span>):</span><br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    <span class="hljs-comment"># name = scrapy.Field()</span><br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>
<p>该类继承自scrapy.Item类，实际上，该类的操作方法和字典一样，不同之处在于，该类使用时需要在items.py中使用<code>scrapy.Field()</code>“声明”一个键值对，再在解析函数中实例化对象来使用。<br><strong>这么做的好处</strong></p>
<ul>
<li>目前已知的好处就是防止手抖将键的名字写错，因为当解析函数中键的名字写错时函数会引发异常。</li>
<li>把数据交给pipeline时，可以用<code>isinstance()</code>来判断数据属于哪个Item，以便进行不同的处理</li>
</ul>
<h2 id="在mongdoDB中使用Item时的注意事项"><a href="#在mongdoDB中使用Item时的注意事项" class="headerlink" title="在mongdoDB中使用Item时的注意事项"></a>在mongdoDB中使用Item时的注意事项</h2><p>由于mongdoDB只支持字典操作，所以要使用dict()函数将Item对象转化为字典类型再储存入数据库中</p>
<hr>
<hr>
<h1 id="Scrapy中的CrawlSpider类"><a href="#Scrapy中的CrawlSpider类" class="headerlink" title="Scrapy中的CrawlSpider类"></a>Scrapy中的CrawlSpider类</h1><h2 id="创建CrawlSpider爬虫"><a href="#创建CrawlSpider爬虫" class="headerlink" title="创建CrawlSpider爬虫"></a>创建CrawlSpider爬虫</h2><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python">scrapy genspider -t crawl name domain<br></code></pre></td></tr></table></figure>
<p>创建后会发现新建爬虫的父类变成了CrawlSpider，并且出现了一个rules的元组，即为规则。<br>rules元组冲包含了Rule类，包含以下参数</p>
<ul>
<li>LinkExtractor，即为链接提取器<ul>
<li>allow 一个正则表达式</li>
</ul>
</li>
<li>callback 提取出的url地址会交给callback处理</li>
<li>follow 当前url地址的响应是否重新经过rules来提取（开启后相当于一个递归，通常在翻页时使用）</li>
</ul>
<p><strong>当然，也可以不使用rules规则，而在parse_item中yield出一个Request类，并定义一个解析方法来对网页进行解析。</strong></p>
<h2 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h2><p>该类为程序员提供了一个创建爬虫的更简便的方式，可以用更少的代码量完成同样的操作，但缺点是无法在解析方法之间传递数据（没有meta参数用于传递）</p>
<p>LinkExtractor参数补充：</p>
<ul>
<li>allow 满足正则表达式的url会被提取</li>
<li>deny 满足正则表达式的url都不会被提取（优先级高于allow）</li>
<li>allow_domains 会被提取的链接的domain</li>
<li>deny_domains 不会被提取的链接的domain</li>
<li>restrict_xpaths 使用xpath表达式，和allow共同作用过滤链接，该xpath满足的url地址都会被提取</li>
</ul>
<p>spider.Rule常见参数</p>
<ul>
<li>Link_extractor：</li>
<li>callback：</li>
<li>follow:</li>
<li>process_link:为一个函数对象，当从Link_extractor中过滤出一个url时会调用这个函数来处理url（一般用于过滤url）</li>
<li>process_request:一个函数对象，当提取到一个request时会调用该函数（一般用于过滤request）</li>
</ul>
<hr>
<hr>
]]></content>
      <tags>
        <tag>前端</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
